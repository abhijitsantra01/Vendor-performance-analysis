import pandas as pd
data = pd.read_csv("news.csv")
data.head()
data.shape
events = data["Event Description"]
# events.head(3)
events[119]
urls = data["URL"]
urls.head(3)
# Make sure you have the required libraries installed:
# pip install transformers
# pip install torch
# pip install sentencepiece
# pip install requests
# pip install beautifulsoup4

from transformers import pipeline
import json
import requests
from bs4 import BeautifulSoup

def get_event_from_url(url, headline):
    """
    Fetches a news article from a URL, extracts the main event using an LLM,
    and returns the details in a JSON object.

    Args:
        url (str): The URL of the news article.
        headline (str): The headline of the news article.

    Returns:
        dict: A dictionary containing the URL, headline, and the extracted event.
              Returns an error dictionary if something goes wrong.
    """
    try:
        # 1. Fetch the article content from the web.
        print(f"Fetching article from: {url}...")
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)

        # 2. Parse the HTML and extract the main article text.
        # This part is generic and might need adjustments for different website structures.
        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        article_text = ' '.join([p.get_text() for p in paragraphs])

        if not article_text:
            return {"error": "Could not find any paragraph text on the page."}

        print("Article text extracted successfully.")

        # 3. Initialize the language model.
        print("Initializing the text-to-text generation model (flan-t5-base)...")
        extractor = pipeline('text2text-generation', model='google/pegasus-cnn_dailymail')
        print("Model initialized successfully.")

        # 4. Create a prompt to ask the model to extract the event.
        prompt = f"""
        Read the following news article and extract the main event described in it.

        Article: "{article_text}"

        Extracted Event:
        """

        # 5. Get the event from the model.
        print("Extracting the news event from the text...")
        outputs = extractor(prompt, max_new_tokens=512, clean_up_tokenization_spaces=True)
        event = outputs[0]['generated_text'].strip()
        print("Event extracted successfully.")

        # 6. Assemble and return the final JSON object.
        result = {
            "news_url": url,
            "news_headline": headline,
            "news_event": event
        }
        return result

    except requests.exceptions.RequestException as e:
        return {"error": f"Failed to fetch the URL: {e}"}
    except Exception as e:
        return {"error": f"An unexpected error occurred: {e}"}

# --- Example Usage ---
if __name__ == "__main__":
   for i in range(120):
    # You can change this URL and headline to any news article.
    sample_url = urls[i]
    sample_headline = events[i]

    # Get the structured event data
    event_data = get_event_from_url(sample_url, sample_headline)

    # Print the final, structured JSON object in a readable format
    print("\n--- Extracted Event (JSON Format) ---")
    print(json.dumps(event_data, indent=4))



from transformers import pipeline, AutoTokenizer
import json
import requests
from bs4 import BeautifulSoup

# Load model and tokenizer once
model_name = 'mrm8488/t5-base-finetuned-summarize-news'
extractor = pipeline('text2text-generation', model=model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
max_length = 512  # max tokens for flan-t5-base

def get_event_from_url(url, headline, extractor, tokenizer, max_length):
    try:
        print(f"Fetching article from: {url}...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        article_text = ' '.join([p.get_text() for p in paragraphs])

        if not article_text:
            return {"news_url": url, "news_headline": headline, "error": "Could not find any paragraph text on the page."}

        # Truncate article text to max tokens for model input
        inputs = tokenizer(article_text, max_length=max_length, truncation=True, return_tensors="pt")
        truncated_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)

        prompt = f"""
        Read the following news article and extract the main event described in it.

        Article: "{truncated_text}"

        Extracted Event:
        """

        outputs = extractor(prompt, max_new_tokens=150)
        event = outputs[0]['generated_text'].strip()

        result = {
            "news_url": url,
            "news_headline": headline,
            "news_event": event
        }
        return result

    except requests.exceptions.RequestException as e:
        return {"news_url": url, "news_headline": headline, "error": f"Failed to fetch the URL: {e}"}
    except Exception as e:
        return {"news_url": url, "news_headline": headline, "error": f"An unexpected error occurred: {e}"}

if __name__ == "__main__":
    # Example lists; replace these with your actual URLs and headlines
    urls = urls
    events = events

    # Validate URLs to include only those with proper scheme
    valid_urls = [url for url in urls if url.startswith('http')]
    valid_headlines = [events[i] for i, url in enumerate(urls) if url.startswith('http')]

    all_event_data = []

    for i, sample_url in enumerate(valid_urls):
        sample_headline = valid_headlines[i]
        event_data = get_event_from_url(sample_url, sample_headline, extractor, tokenizer, max_length)
        print(json.dumps(event_data, indent=4))
        all_event_data.append(event_data)

    # Save all event data to JSON file
    with open("extracted_events.json", "w", encoding="utf-8") as f:
        json.dump(all_event_data, f, ensure_ascii=False, indent=2)

    print("All extracted events saved to extracted_events.json")
